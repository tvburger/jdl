<!DOCTYPE HTML>
<html lang>
<head>
<!-- Generated by javadoc (24) -->
<title>Source code</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="source: package: net.tvburger.jdl.model.nn.training.optimizers, class: NeuralNetworkOptimizers">
<meta name="generator" content="javadoc/SourceToHTMLConverter">
<link rel="stylesheet" type="text/css" href="../../../../../../../../resource-files/stylesheet.css" title="Style">
</head>
<body class="source-page">
<main role="main">
<div class="source-container">
<pre><span class="source-line-no">001</span><span id="line-1">package net.tvburger.jdl.model.nn.training.optimizers;</span>
<span class="source-line-no">002</span><span id="line-2"></span>
<span class="source-line-no">003</span><span id="line-3">import net.tvburger.jdl.common.patterns.StaticUtility;</span>
<span class="source-line-no">004</span><span id="line-4">import net.tvburger.jdl.model.nn.NeuralNetwork;</span>
<span class="source-line-no">005</span><span id="line-5">import net.tvburger.jdl.model.training.optimizer.GradientDescentOptimizer;</span>
<span class="source-line-no">006</span><span id="line-6">import net.tvburger.jdl.model.training.optimizer.steps.*;</span>
<span class="source-line-no">007</span><span id="line-7"></span>
<span class="source-line-no">008</span><span id="line-8">@StaticUtility</span>
<span class="source-line-no">009</span><span id="line-9">public final class NeuralNetworkOptimizers {</span>
<span class="source-line-no">010</span><span id="line-10"></span>
<span class="source-line-no">011</span><span id="line-11">    private static final BackPropagation BACK_PROPAGATION = new BackPropagation();</span>
<span class="source-line-no">012</span><span id="line-12"></span>
<span class="source-line-no">013</span><span id="line-13">    public static final float DEFAULT_LEARNING_RATE = 0.1f;</span>
<span class="source-line-no">014</span><span id="line-14">    public static final float DEFAULT_ADAPTIVE_BETA = 0.9f;</span>
<span class="source-line-no">015</span><span id="line-15">    public static final float DEFAULT_MOMENTUM_BETA = 0.999f;</span>
<span class="source-line-no">016</span><span id="line-16">    public static final float DEFAULT_LAMBDA = 0.001f;</span>
<span class="source-line-no">017</span><span id="line-17"></span>
<span class="source-line-no">018</span><span id="line-18">    private NeuralNetworkOptimizers() {</span>
<span class="source-line-no">019</span><span id="line-19">    }</span>
<span class="source-line-no">020</span><span id="line-20"></span>
<span class="source-line-no">021</span><span id="line-21">    public static GradientDescentOptimizer&lt;NeuralNetwork, Float&gt; vanilla() {</span>
<span class="source-line-no">022</span><span id="line-22">        return vanilla(DEFAULT_LEARNING_RATE);</span>
<span class="source-line-no">023</span><span id="line-23">    }</span>
<span class="source-line-no">024</span><span id="line-24"></span>
<span class="source-line-no">025</span><span id="line-25">    public static GradientDescentOptimizer&lt;NeuralNetwork, Float&gt; vanilla(float learningRate) {</span>
<span class="source-line-no">026</span><span id="line-26">        return new GradientDescentOptimizer&lt;&gt;(BACK_PROPAGATION, new VanillaGradientDescent&lt;&gt;(learningRate));</span>
<span class="source-line-no">027</span><span id="line-27">    }</span>
<span class="source-line-no">028</span><span id="line-28"></span>
<span class="source-line-no">029</span><span id="line-29">    public static GradientDescentOptimizer&lt;NeuralNetwork, Float&gt; adaGrad() {</span>
<span class="source-line-no">030</span><span id="line-30">        return adaGrad(DEFAULT_LEARNING_RATE);</span>
<span class="source-line-no">031</span><span id="line-31">    }</span>
<span class="source-line-no">032</span><span id="line-32"></span>
<span class="source-line-no">033</span><span id="line-33">    public static GradientDescentOptimizer&lt;NeuralNetwork, Float&gt; adaGrad(float learningRate) {</span>
<span class="source-line-no">034</span><span id="line-34">        return new GradientDescentOptimizer&lt;&gt;(BACK_PROPAGATION, new AdaGrad&lt;&gt;(learningRate));</span>
<span class="source-line-no">035</span><span id="line-35">    }</span>
<span class="source-line-no">036</span><span id="line-36"></span>
<span class="source-line-no">037</span><span id="line-37">    public static GradientDescentOptimizer&lt;NeuralNetwork, Float&gt; rmsProp() {</span>
<span class="source-line-no">038</span><span id="line-38">        return rmsProp(DEFAULT_LEARNING_RATE, DEFAULT_ADAPTIVE_BETA);</span>
<span class="source-line-no">039</span><span id="line-39">    }</span>
<span class="source-line-no">040</span><span id="line-40"></span>
<span class="source-line-no">041</span><span id="line-41">    public static GradientDescentOptimizer&lt;NeuralNetwork, Float&gt; rmsProp(float learningRate, float beta) {</span>
<span class="source-line-no">042</span><span id="line-42">        return new GradientDescentOptimizer&lt;&gt;(BACK_PROPAGATION, new RMSProp&lt;&gt;(learningRate, beta));</span>
<span class="source-line-no">043</span><span id="line-43">    }</span>
<span class="source-line-no">044</span><span id="line-44"></span>
<span class="source-line-no">045</span><span id="line-45">    public static GradientDescentOptimizer&lt;NeuralNetwork, Float&gt; adam() {</span>
<span class="source-line-no">046</span><span id="line-46">        return adam(DEFAULT_LEARNING_RATE, DEFAULT_ADAPTIVE_BETA, DEFAULT_MOMENTUM_BETA);</span>
<span class="source-line-no">047</span><span id="line-47">    }</span>
<span class="source-line-no">048</span><span id="line-48"></span>
<span class="source-line-no">049</span><span id="line-49">    public static GradientDescentOptimizer&lt;NeuralNetwork, Float&gt; adam(float learningRate, float beta1, float beta2) {</span>
<span class="source-line-no">050</span><span id="line-50">        return new GradientDescentOptimizer&lt;&gt;(BACK_PROPAGATION, new Adam&lt;&gt;(learningRate, beta1, beta2));</span>
<span class="source-line-no">051</span><span id="line-51">    }</span>
<span class="source-line-no">052</span><span id="line-52"></span>
<span class="source-line-no">053</span><span id="line-53">    public static GradientDescentOptimizer&lt;NeuralNetwork, Float&gt; adamW() {</span>
<span class="source-line-no">054</span><span id="line-54">        return adamW(DEFAULT_LEARNING_RATE, DEFAULT_ADAPTIVE_BETA, DEFAULT_MOMENTUM_BETA, DEFAULT_LAMBDA);</span>
<span class="source-line-no">055</span><span id="line-55">    }</span>
<span class="source-line-no">056</span><span id="line-56"></span>
<span class="source-line-no">057</span><span id="line-57">    public static GradientDescentOptimizer&lt;NeuralNetwork, Float&gt; adamW(float learningRate, float beta1, float beta2, float lambda) {</span>
<span class="source-line-no">058</span><span id="line-58">        return new GradientDescentOptimizer&lt;&gt;(BACK_PROPAGATION, new AdamW&lt;&gt;(learningRate, beta1, beta2, lambda));</span>
<span class="source-line-no">059</span><span id="line-59">    }</span>
<span class="source-line-no">060</span><span id="line-60">}</span>




























































</pre>
</div>
</main>
</body>
</html>
