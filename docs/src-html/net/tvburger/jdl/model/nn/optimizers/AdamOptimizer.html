<!DOCTYPE HTML>
<html lang>
<head>
<!-- Generated by javadoc (24) -->
<title>Source code</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="source: package: net.tvburger.jdl.model.nn.optimizers, class: AdamOptimizer">
<meta name="generator" content="javadoc/SourceToHTMLConverter">
<link rel="stylesheet" type="text/css" href="../../../../../../../resource-files/stylesheet.css" title="Style">
</head>
<body class="source-page">
<main role="main">
<div class="source-container">
<pre><span class="source-line-no">001</span><span id="line-1">package net.tvburger.jdl.model.nn.optimizers;</span>
<span class="source-line-no">002</span><span id="line-2"></span>
<span class="source-line-no">003</span><span id="line-3">import net.tvburger.jdl.common.patterns.Strategy;</span>
<span class="source-line-no">004</span><span id="line-4">import net.tvburger.jdl.model.DataSet;</span>
<span class="source-line-no">005</span><span id="line-5">import net.tvburger.jdl.model.nn.ActivationsCachedNeuron;</span>
<span class="source-line-no">006</span><span id="line-6">import net.tvburger.jdl.model.nn.NeuralNetwork;</span>
<span class="source-line-no">007</span><span id="line-7">import net.tvburger.jdl.model.nn.Neuron;</span>
<span class="source-line-no">008</span><span id="line-8">import net.tvburger.jdl.model.training.ObjectiveFunction;</span>
<span class="source-line-no">009</span><span id="line-9">import net.tvburger.jdl.model.training.Optimizer;</span>
<span class="source-line-no">010</span><span id="line-10"></span>
<span class="source-line-no">011</span><span id="line-11">import java.util.IdentityHashMap;</span>
<span class="source-line-no">012</span><span id="line-12">import java.util.Map;</span>
<span class="source-line-no">013</span><span id="line-13"></span>
<span class="source-line-no">014</span><span id="line-14">/**</span>
<span class="source-line-no">015</span><span id="line-15"> * Adam optimizer (Kingma &amp;amp; Ba, 2014).</span>
<span class="source-line-no">016</span><span id="line-16"> * Mirrors GradientDescent's training loop: we accumulate per-batch gradients,</span>
<span class="source-line-no">017</span><span id="line-17"> * then apply Adam updates with bias correction.</span>
<span class="source-line-no">018</span><span id="line-18"> */</span>
<span class="source-line-no">019</span><span id="line-19">@Strategy(Strategy.Role.CONCRETE)</span>
<span class="source-line-no">020</span><span id="line-20">public class AdamOptimizer&lt;N extends NeuralNetwork&gt; implements Optimizer&lt;N&gt; {</span>
<span class="source-line-no">021</span><span id="line-21"></span>
<span class="source-line-no">022</span><span id="line-22">    public static final float DEFAULT_LEARNING_RATE = 1e-3f;   // alpha</span>
<span class="source-line-no">023</span><span id="line-23">    public static final float DEFAULT_BETA1 = 0.9f;</span>
<span class="source-line-no">024</span><span id="line-24">    public static final float DEFAULT_BETA2 = 0.999f;</span>
<span class="source-line-no">025</span><span id="line-25">    public static final float DEFAULT_EPSILON = 1e-8f;</span>
<span class="source-line-no">026</span><span id="line-26"></span>
<span class="source-line-no">027</span><span id="line-27">    private float learningRate = DEFAULT_LEARNING_RATE;</span>
<span class="source-line-no">028</span><span id="line-28">    private float beta1 = DEFAULT_BETA1;</span>
<span class="source-line-no">029</span><span id="line-29">    private float beta2 = DEFAULT_BETA2;</span>
<span class="source-line-no">030</span><span id="line-30">    private float epsilon = DEFAULT_EPSILON;</span>
<span class="source-line-no">031</span><span id="line-31"></span>
<span class="source-line-no">032</span><span id="line-32">    // Adam state: first (m) and second (v) moments for weights and biases</span>
<span class="source-line-no">033</span><span id="line-33">    private final Map&lt;Neuron, float[]&gt; mW = new IdentityHashMap&lt;&gt;();</span>
<span class="source-line-no">034</span><span id="line-34">    private final Map&lt;Neuron, float[]&gt; vW = new IdentityHashMap&lt;&gt;();</span>
<span class="source-line-no">035</span><span id="line-35">    private final Map&lt;Neuron, Float&gt; mB = new IdentityHashMap&lt;&gt;();</span>
<span class="source-line-no">036</span><span id="line-36">    private final Map&lt;Neuron, Float&gt; vB = new IdentityHashMap&lt;&gt;();</span>
<span class="source-line-no">037</span><span id="line-37"></span>
<span class="source-line-no">038</span><span id="line-38">    // Global step for bias correction</span>
<span class="source-line-no">039</span><span id="line-39">    private long t = 0L;</span>
<span class="source-line-no">040</span><span id="line-40"></span>
<span class="source-line-no">041</span><span id="line-41">    public float getLearningRate() {</span>
<span class="source-line-no">042</span><span id="line-42">        return learningRate;</span>
<span class="source-line-no">043</span><span id="line-43">    }</span>
<span class="source-line-no">044</span><span id="line-44"></span>
<span class="source-line-no">045</span><span id="line-45">    public void setLearningRate(float learningRate) {</span>
<span class="source-line-no">046</span><span id="line-46">        this.learningRate = learningRate;</span>
<span class="source-line-no">047</span><span id="line-47">    }</span>
<span class="source-line-no">048</span><span id="line-48"></span>
<span class="source-line-no">049</span><span id="line-49">    public float getBeta1() {</span>
<span class="source-line-no">050</span><span id="line-50">        return beta1;</span>
<span class="source-line-no">051</span><span id="line-51">    }</span>
<span class="source-line-no">052</span><span id="line-52"></span>
<span class="source-line-no">053</span><span id="line-53">    public void setBeta1(float beta1) {</span>
<span class="source-line-no">054</span><span id="line-54">        this.beta1 = beta1;</span>
<span class="source-line-no">055</span><span id="line-55">    }</span>
<span class="source-line-no">056</span><span id="line-56"></span>
<span class="source-line-no">057</span><span id="line-57">    public float getBeta2() {</span>
<span class="source-line-no">058</span><span id="line-58">        return beta2;</span>
<span class="source-line-no">059</span><span id="line-59">    }</span>
<span class="source-line-no">060</span><span id="line-60"></span>
<span class="source-line-no">061</span><span id="line-61">    public void setBeta2(float beta2) {</span>
<span class="source-line-no">062</span><span id="line-62">        this.beta2 = beta2;</span>
<span class="source-line-no">063</span><span id="line-63">    }</span>
<span class="source-line-no">064</span><span id="line-64"></span>
<span class="source-line-no">065</span><span id="line-65">    public float getEpsilon() {</span>
<span class="source-line-no">066</span><span id="line-66">        return epsilon;</span>
<span class="source-line-no">067</span><span id="line-67">    }</span>
<span class="source-line-no">068</span><span id="line-68"></span>
<span class="source-line-no">069</span><span id="line-69">    public void setEpsilon(float epsilon) {</span>
<span class="source-line-no">070</span><span id="line-70">        this.epsilon = epsilon;</span>
<span class="source-line-no">071</span><span id="line-71">    }</span>
<span class="source-line-no">072</span><span id="line-72"></span>
<span class="source-line-no">073</span><span id="line-73">    @Override</span>
<span class="source-line-no">074</span><span id="line-74">    public void optimize(N neuralNetwork, DataSet trainingSet, ObjectiveFunction objective) {</span>
<span class="source-line-no">075</span><span id="line-75">        final int batchSize = trainingSet.samples().size();</span>
<span class="source-line-no">076</span><span id="line-76">        if (batchSize == 0) return;</span>
<span class="source-line-no">077</span><span id="line-77"></span>
<span class="source-line-no">078</span><span id="line-78">        final int depth = neuralNetwork.getDepth();</span>
<span class="source-line-no">079</span><span id="line-79"></span>
<span class="source-line-no">080</span><span id="line-80">        // ---- Accumulate per-batch gradients (same as your GD) ----</span>
<span class="source-line-no">081</span><span id="line-81">        final Map&lt;Neuron, float[]&gt; dWsum = new IdentityHashMap&lt;&gt;();</span>
<span class="source-line-no">082</span><span id="line-82">        final Map&lt;Neuron, Float&gt; dBsum = new IdentityHashMap&lt;&gt;();</span>
<span class="source-line-no">083</span><span id="line-83"></span>
<span class="source-line-no">084</span><span id="line-84">        for (int i = 0; i &lt; batchSize; i++) {</span>
<span class="source-line-no">085</span><span id="line-85">            final Map&lt;Neuron, Float&gt; deltaN = new IdentityHashMap&lt;&gt;();</span>
<span class="source-line-no">086</span><span id="line-86"></span>
<span class="source-line-no">087</span><span id="line-87">            // Output layer deltas</span>
<span class="source-line-no">088</span><span id="line-88">            int outWidth = neuralNetwork.coArity();</span>
<span class="source-line-no">089</span><span id="line-89">            DataSet.Sample sample = trainingSet.samples().get(i);</span>
<span class="source-line-no">090</span><span id="line-90">            float[] estimated = neuralNetwork.estimate(sample.features());</span>
<span class="source-line-no">091</span><span id="line-91">            float[] lossGradients = objective.calculateGradient_dJ_da(batchSize, estimated, sample.targetOutputs());</span>
<span class="source-line-no">092</span><span id="line-92"></span>
<span class="source-line-no">093</span><span id="line-93">            for (int j = 0; j &lt; outWidth; j++) {</span>
<span class="source-line-no">094</span><span id="line-94">                ActivationsCachedNeuron out = neuralNetwork.getNeuron(depth, j, ActivationsCachedNeuron.class);</span>
<span class="source-line-no">095</span><span id="line-95">                ActivationsCachedNeuron.Activation activation = out.getCache().get(i);</span>
<span class="source-line-no">096</span><span id="line-96"></span>
<span class="source-line-no">097</span><span id="line-97">                float delta = lossGradients[j] * activation.gradient();</span>
<span class="source-line-no">098</span><span id="line-98">                deltaN.put(out, delta);</span>
<span class="source-line-no">099</span><span id="line-99"></span>
<span class="source-line-no">100</span><span id="line-100">                float[] inputs = activation.inputs();</span>
<span class="source-line-no">101</span><span id="line-101">                float[] gsum = dWsum.computeIfAbsent(out, k -&gt; new float[inputs.length]);</span>
<span class="source-line-no">102</span><span id="line-102">                for (int d = 0; d &lt; inputs.length; d++) gsum[d] += delta * inputs[d];</span>
<span class="source-line-no">103</span><span id="line-103">                dBsum.merge(out, delta, Float::sum);</span>
<span class="source-line-no">104</span><span id="line-104">            }</span>
<span class="source-line-no">105</span><span id="line-105"></span>
<span class="source-line-no">106</span><span id="line-106">            // Hidden layers deltas (backward)</span>
<span class="source-line-no">107</span><span id="line-107">            for (int l = depth - 1; l &gt;= 1; l--) {</span>
<span class="source-line-no">108</span><span id="line-108">                int width = neuralNetwork.getWidth(l);</span>
<span class="source-line-no">109</span><span id="line-109">                for (int j = 0; j &lt; width; j++) {</span>
<span class="source-line-no">110</span><span id="line-110">                    ActivationsCachedNeuron neuron = neuralNetwork.getNeuron(l, j, ActivationsCachedNeuron.class);</span>
<span class="source-line-no">111</span><span id="line-111"></span>
<span class="source-line-no">112</span><span id="line-112">                    float back = 0f;</span>
<span class="source-line-no">113</span><span id="line-113">                    Map&lt;Neuron, Float&gt; outs = neuralNetwork.getOutputConnections(l, j);</span>
<span class="source-line-no">114</span><span id="line-114">                    for (Map.Entry&lt;Neuron, Float&gt; e : outs.entrySet()) {</span>
<span class="source-line-no">115</span><span id="line-115">                        Float deltaNext = deltaN.get(e.getKey());</span>
<span class="source-line-no">116</span><span id="line-116">                        if (deltaNext != null) back += deltaNext * e.getValue();</span>
<span class="source-line-no">117</span><span id="line-117">                    }</span>
<span class="source-line-no">118</span><span id="line-118"></span>
<span class="source-line-no">119</span><span id="line-119">                    ActivationsCachedNeuron.Activation activation = neuron.getCache().get(i);</span>
<span class="source-line-no">120</span><span id="line-120">                    float delta = back * activation.gradient();</span>
<span class="source-line-no">121</span><span id="line-121">                    deltaN.put(neuron, delta);</span>
<span class="source-line-no">122</span><span id="line-122"></span>
<span class="source-line-no">123</span><span id="line-123">                    float[] inputs = activation.inputs();</span>
<span class="source-line-no">124</span><span id="line-124">                    float[] gsum = dWsum.computeIfAbsent(neuron, k -&gt; new float[inputs.length]);</span>
<span class="source-line-no">125</span><span id="line-125">                    for (int d = 0; d &lt; inputs.length; d++) gsum[d] += delta * inputs[d];</span>
<span class="source-line-no">126</span><span id="line-126">                    dBsum.merge(neuron, delta, Float::sum);</span>
<span class="source-line-no">127</span><span id="line-127">                }</span>
<span class="source-line-no">128</span><span id="line-128">            }</span>
<span class="source-line-no">129</span><span id="line-129">        }</span>
<span class="source-line-no">130</span><span id="line-130"></span>
<span class="source-line-no">131</span><span id="line-131">        // ---- Adam update on averaged gradients ----</span>
<span class="source-line-no">132</span><span id="line-132">        t += 1L; // step</span>
<span class="source-line-no">133</span><span id="line-133">        final float invN = 1f / batchSize;</span>
<span class="source-line-no">134</span><span id="line-134"></span>
<span class="source-line-no">135</span><span id="line-135">        // Precompute bias correction factors</span>
<span class="source-line-no">136</span><span id="line-136">        final double b1t = Math.pow(beta1, t);</span>
<span class="source-line-no">137</span><span id="line-137">        final double b2t = Math.pow(beta2, t);</span>
<span class="source-line-no">138</span><span id="line-138">        final float biasCorr1 = (float) (1.0 / (1.0 - b1t));</span>
<span class="source-line-no">139</span><span id="line-139">        final float biasCorr2 = (float) (1.0 / (1.0 - b2t));</span>
<span class="source-line-no">140</span><span id="line-140"></span>
<span class="source-line-no">141</span><span id="line-141">        for (int l = 1; l &lt;= depth; l++) {</span>
<span class="source-line-no">142</span><span id="line-142">            int width = neuralNetwork.getWidth(l);</span>
<span class="source-line-no">143</span><span id="line-143">            for (int j = 0; j &lt; width; j++) {</span>
<span class="source-line-no">144</span><span id="line-144">                ActivationsCachedNeuron neuron = neuralNetwork.getNeuron(l, j, ActivationsCachedNeuron.class);</span>
<span class="source-line-no">145</span><span id="line-145"></span>
<span class="source-line-no">146</span><span id="line-146">                // Weights</span>
<span class="source-line-no">147</span><span id="line-147">                float[] w = neuron.getWeights();</span>
<span class="source-line-no">148</span><span id="line-148">                float[] gWsum = dWsum.get(neuron);</span>
<span class="source-line-no">149</span><span id="line-149"></span>
<span class="source-line-no">150</span><span id="line-150">                if (gWsum != null) {</span>
<span class="source-line-no">151</span><span id="line-151">                    // Ensure moment buffers exist and match shape</span>
<span class="source-line-no">152</span><span id="line-152">                    float[] mWi = mW.computeIfAbsent(neuron, k -&gt; new float[w.length]);</span>
<span class="source-line-no">153</span><span id="line-153">                    float[] vWi = vW.computeIfAbsent(neuron, k -&gt; new float[w.length]);</span>
<span class="source-line-no">154</span><span id="line-154">                    if (mWi.length != w.length) {</span>
<span class="source-line-no">155</span><span id="line-155">                        mWi = new float[w.length];</span>
<span class="source-line-no">156</span><span id="line-156">                        vWi = new float[w.length];</span>
<span class="source-line-no">157</span><span id="line-157">                        mW.put(neuron, mWi);</span>
<span class="source-line-no">158</span><span id="line-158">                        vW.put(neuron, vWi);</span>
<span class="source-line-no">159</span><span id="line-159">                    }</span>
<span class="source-line-no">160</span><span id="line-160"></span>
<span class="source-line-no">161</span><span id="line-161">                    for (int d = 0; d &lt; w.length; d++) {</span>
<span class="source-line-no">162</span><span id="line-162">                        // Average gradient over batch</span>
<span class="source-line-no">163</span><span id="line-163">                        float g = invN * gWsum[d];</span>
<span class="source-line-no">164</span><span id="line-164"></span>
<span class="source-line-no">165</span><span id="line-165">                        // Adam moment updates</span>
<span class="source-line-no">166</span><span id="line-166">                        mWi[d] = beta1 * mWi[d] + (1 - beta1) * g;</span>
<span class="source-line-no">167</span><span id="line-167">                        vWi[d] = beta2 * vWi[d] + (1 - beta2) * (g * g);</span>
<span class="source-line-no">168</span><span id="line-168"></span>
<span class="source-line-no">169</span><span id="line-169">                        // Bias-corrected moments</span>
<span class="source-line-no">170</span><span id="line-170">                        float mHat = mWi[d] * biasCorr1;</span>
<span class="source-line-no">171</span><span id="line-171">                        float vHat = vWi[d] * biasCorr2;</span>
<span class="source-line-no">172</span><span id="line-172"></span>
<span class="source-line-no">173</span><span id="line-173">                        // Parameter update</span>
<span class="source-line-no">174</span><span id="line-174">                        w[d] -= learningRate * (mHat / (float) (Math.sqrt(vHat) + (double) epsilon));</span>
<span class="source-line-no">175</span><span id="line-175">                    }</span>
<span class="source-line-no">176</span><span id="line-176">                }</span>
<span class="source-line-no">177</span><span id="line-177"></span>
<span class="source-line-no">178</span><span id="line-178">                // Bias</span>
<span class="source-line-no">179</span><span id="line-179">                Float gBsum = dBsum.get(neuron);</span>
<span class="source-line-no">180</span><span id="line-180">                if (gBsum != null) {</span>
<span class="source-line-no">181</span><span id="line-181">                    float g = invN * gBsum;</span>
<span class="source-line-no">182</span><span id="line-182"></span>
<span class="source-line-no">183</span><span id="line-183">                    float mBi = mB.getOrDefault(neuron, 0f);</span>
<span class="source-line-no">184</span><span id="line-184">                    float vBi = vB.getOrDefault(neuron, 0f);</span>
<span class="source-line-no">185</span><span id="line-185"></span>
<span class="source-line-no">186</span><span id="line-186">                    mBi = beta1 * mBi + (1 - beta1) * g;</span>
<span class="source-line-no">187</span><span id="line-187">                    vBi = beta2 * vBi + (1 - beta2) * (g * g);</span>
<span class="source-line-no">188</span><span id="line-188"></span>
<span class="source-line-no">189</span><span id="line-189">                    float mHat = mBi * biasCorr1;</span>
<span class="source-line-no">190</span><span id="line-190">                    float vHat = vBi * biasCorr2;</span>
<span class="source-line-no">191</span><span id="line-191"></span>
<span class="source-line-no">192</span><span id="line-192">                    float b = neuron.getBias();</span>
<span class="source-line-no">193</span><span id="line-193">                    b -= learningRate * (mHat / (float) (Math.sqrt(vHat) + (double) epsilon));</span>
<span class="source-line-no">194</span><span id="line-194">                    neuron.setBias(b);</span>
<span class="source-line-no">195</span><span id="line-195"></span>
<span class="source-line-no">196</span><span id="line-196">                    mB.put(neuron, mBi);</span>
<span class="source-line-no">197</span><span id="line-197">                    vB.put(neuron, vBi);</span>
<span class="source-line-no">198</span><span id="line-198">                }</span>
<span class="source-line-no">199</span><span id="line-199"></span>
<span class="source-line-no">200</span><span id="line-200">                // Clear per-batch caches like in GD</span>
<span class="source-line-no">201</span><span id="line-201">                neuron.clearCache();</span>
<span class="source-line-no">202</span><span id="line-202">            }</span>
<span class="source-line-no">203</span><span id="line-203">        }</span>
<span class="source-line-no">204</span><span id="line-204">    }</span>
<span class="source-line-no">205</span><span id="line-205">}</span>




























































</pre>
</div>
</main>
</body>
</html>
