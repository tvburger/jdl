<!DOCTYPE HTML>
<html lang>
<head>
<!-- Generated by javadoc (24) -->
<title>Source code</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="source: package: net.tvburger.jdl.model.nn.optimizers, class: StochasticGradientDescent">
<meta name="generator" content="javadoc/SourceToHTMLConverter">
<link rel="stylesheet" type="text/css" href="../../../../../../../resource-files/stylesheet.css" title="Style">
</head>
<body class="source-page">
<main role="main">
<div class="source-container">
<pre><span class="source-line-no">001</span><span id="line-1">package net.tvburger.jdl.model.nn.optimizers;</span>
<span class="source-line-no">002</span><span id="line-2"></span>
<span class="source-line-no">003</span><span id="line-3">import net.tvburger.jdl.common.patterns.Strategy;</span>
<span class="source-line-no">004</span><span id="line-4">import net.tvburger.jdl.model.DataSet;</span>
<span class="source-line-no">005</span><span id="line-5">import net.tvburger.jdl.model.nn.ActivationsCachedNeuron;</span>
<span class="source-line-no">006</span><span id="line-6">import net.tvburger.jdl.model.nn.NeuralNetwork;</span>
<span class="source-line-no">007</span><span id="line-7">import net.tvburger.jdl.model.nn.Neuron;</span>
<span class="source-line-no">008</span><span id="line-8">import net.tvburger.jdl.model.training.ObjectiveFunction;</span>
<span class="source-line-no">009</span><span id="line-9">import net.tvburger.jdl.model.training.Optimizer;</span>
<span class="source-line-no">010</span><span id="line-10"></span>
<span class="source-line-no">011</span><span id="line-11">import java.util.IdentityHashMap;</span>
<span class="source-line-no">012</span><span id="line-12">import java.util.Map;</span>
<span class="source-line-no">013</span><span id="line-13"></span>
<span class="source-line-no">014</span><span id="line-14">@Strategy(role = Strategy.Role.CONCRETE)</span>
<span class="source-line-no">015</span><span id="line-15">public class StochasticGradientDescent&lt;N extends NeuralNetwork&gt; implements Optimizer&lt;N&gt; {</span>
<span class="source-line-no">016</span><span id="line-16"></span>
<span class="source-line-no">017</span><span id="line-17">    public static final float DEFAULT_LEARNING_RATE = 0.1f;</span>
<span class="source-line-no">018</span><span id="line-18"></span>
<span class="source-line-no">019</span><span id="line-19">    private float learningRate = DEFAULT_LEARNING_RATE;</span>
<span class="source-line-no">020</span><span id="line-20"></span>
<span class="source-line-no">021</span><span id="line-21">    public void setLearningRate(float learningRate) {</span>
<span class="source-line-no">022</span><span id="line-22">        this.learningRate = learningRate;</span>
<span class="source-line-no">023</span><span id="line-23">    }</span>
<span class="source-line-no">024</span><span id="line-24"></span>
<span class="source-line-no">025</span><span id="line-25">    public float getLearningRate() {</span>
<span class="source-line-no">026</span><span id="line-26">        return learningRate;</span>
<span class="source-line-no">027</span><span id="line-27">    }</span>
<span class="source-line-no">028</span><span id="line-28"></span>
<span class="source-line-no">029</span><span id="line-29">    @Override</span>
<span class="source-line-no">030</span><span id="line-30">    public void optimize(N neuralNetwork, DataSet trainingSet, ObjectiveFunction objective) {</span>
<span class="source-line-no">031</span><span id="line-31">        int sampleCount = trainingSet.samples().size();</span>
<span class="source-line-no">032</span><span id="line-32">        if (sampleCount == 0) {</span>
<span class="source-line-no">033</span><span id="line-33">            return;</span>
<span class="source-line-no">034</span><span id="line-34">        }</span>
<span class="source-line-no">035</span><span id="line-35">        int depth = neuralNetwork.getDepth();</span>
<span class="source-line-no">036</span><span id="line-36"></span>
<span class="source-line-no">037</span><span id="line-37">        // Allocate accumulators for gradients (sum over batch)</span>
<span class="source-line-no">038</span><span id="line-38">        final Map&lt;Neuron, float[]&gt; dWsum = new IdentityHashMap&lt;&gt;();</span>
<span class="source-line-no">039</span><span id="line-39">        final Map&lt;Neuron, Float&gt; dBsum = new IdentityHashMap&lt;&gt;();</span>
<span class="source-line-no">040</span><span id="line-40"></span>
<span class="source-line-no">041</span><span id="line-41">        // For each sample n, compute error signal δ - delta - layer-by-layer (backward), accumulate gradients</span>
<span class="source-line-no">042</span><span id="line-42">        for (int n = 0; n &lt; sampleCount; n++) {</span>
<span class="source-line-no">043</span><span id="line-43"></span>
<span class="source-line-no">044</span><span id="line-44">            // 3a) Per-sample δ storage for this backward sweep</span>
<span class="source-line-no">045</span><span id="line-45">            final Map&lt;Neuron, Float&gt; deltaN = new IdentityHashMap&lt;&gt;();</span>
<span class="source-line-no">046</span><span id="line-46"></span>
<span class="source-line-no">047</span><span id="line-47">            // 3b) OUTPUT LAYER δ</span>
<span class="source-line-no">048</span><span id="line-48">            int outWidth = neuralNetwork.coArity();</span>
<span class="source-line-no">049</span><span id="line-49">            DataSet.Sample sample = trainingSet.samples().get(n);</span>
<span class="source-line-no">050</span><span id="line-50">            float[] estimated = neuralNetwork.estimate(sample.features());</span>
<span class="source-line-no">051</span><span id="line-51">            float[] lossGradients = objective.determineGradients(estimated, sample.targetOutputs());</span>
<span class="source-line-no">052</span><span id="line-52"></span>
<span class="source-line-no">053</span><span id="line-53">            // we calculate the δ for each output node</span>
<span class="source-line-no">054</span><span id="line-54">            for (int j = 0; j &lt; outWidth; j++) {</span>
<span class="source-line-no">055</span><span id="line-55">                ActivationsCachedNeuron out = neuralNetwork.getNeuron(depth, j, ActivationsCachedNeuron.class);</span>
<span class="source-line-no">056</span><span id="line-56"></span>
<span class="source-line-no">057</span><span id="line-57">                // Fetch cached data for this sample</span>
<span class="source-line-no">058</span><span id="line-58">                ActivationsCachedNeuron.Activation activation = out.getCache().get(n);</span>
<span class="source-line-no">059</span><span id="line-59">                float delta = lossGradients[j] * activation.gradient();</span>
<span class="source-line-no">060</span><span id="line-60">                deltaN.put(out, delta);</span>
<span class="source-line-no">061</span><span id="line-61"></span>
<span class="source-line-no">062</span><span id="line-62">                // Accumulate grads for this neuron: dW += δ * inputs^{(n)}, dB += δ</span>
<span class="source-line-no">063</span><span id="line-63">                float[] inputs = activation.inputs();  // a_i^{(n)} (inputs into this neuron)</span>
<span class="source-line-no">064</span><span id="line-64">                float[] gradientSumPerWeight = dWsum.computeIfAbsent(out, k -&gt; new float[inputs.length]);</span>
<span class="source-line-no">065</span><span id="line-65">                for (int i = 0; i &lt; inputs.length; i++) gradientSumPerWeight[i] += delta * inputs[i];</span>
<span class="source-line-no">066</span><span id="line-66">                dBsum.merge(out, delta, Float::sum);</span>
<span class="source-line-no">067</span><span id="line-67">            }</span>
<span class="source-line-no">068</span><span id="line-68"></span>
<span class="source-line-no">069</span><span id="line-69">            // 3c) HIDDEN LAYERS δ: for l = L-1 .. 1</span>
<span class="source-line-no">070</span><span id="line-70">            for (int layer = depth - 1; layer &gt;= 1; layer--) {</span>
<span class="source-line-no">071</span><span id="line-71">                int width = neuralNetwork.getWidth(layer);</span>
<span class="source-line-no">072</span><span id="line-72">                for (int j = 0; j &lt; width; j++) {</span>
<span class="source-line-no">073</span><span id="line-73">                    ActivationsCachedNeuron neuron = neuralNetwork.getNeuron(layer, j, ActivationsCachedNeuron.class);</span>
<span class="source-line-no">074</span><span id="line-74"></span>
<span class="source-line-no">075</span><span id="line-75">                    // sum_k w_jk * δ_k^{(n)} from the NEXT layer (already computed this sample)</span>
<span class="source-line-no">076</span><span id="line-76">                    float back = 0f;</span>
<span class="source-line-no">077</span><span id="line-77">                    Map&lt;Neuron, Float&gt; outs = neuralNetwork.getOutputConnections(layer, j);</span>
<span class="source-line-no">078</span><span id="line-78">                    for (Map.Entry&lt;Neuron, Float&gt; e : outs.entrySet()) {</span>
<span class="source-line-no">079</span><span id="line-79">                        Float deltaNext = deltaN.get(e.getKey());</span>
<span class="source-line-no">080</span><span id="line-80">                        if (deltaNext != null) back += deltaNext * e.getValue();</span>
<span class="source-line-no">081</span><span id="line-81">                    }</span>
<span class="source-line-no">082</span><span id="line-82"></span>
<span class="source-line-no">083</span><span id="line-83">                    // multiply by local activation derivative f'(z^{(n)})</span>
<span class="source-line-no">084</span><span id="line-84">                    ActivationsCachedNeuron.Activation activation = neuron.getCache().get(n);</span>
<span class="source-line-no">085</span><span id="line-85">                    float fprime = activation.gradient();</span>
<span class="source-line-no">086</span><span id="line-86">                    float delta = back * fprime;</span>
<span class="source-line-no">087</span><span id="line-87">                    deltaN.put(neuron, delta);</span>
<span class="source-line-no">088</span><span id="line-88"></span>
<span class="source-line-no">089</span><span id="line-89">                    // Accumulate grads for this neuron</span>
<span class="source-line-no">090</span><span id="line-90">                    float[] inputs = activation.inputs(); // a_i^{(n)}</span>
<span class="source-line-no">091</span><span id="line-91">                    float[] gradientSumPerWeight = dWsum.computeIfAbsent(neuron, k -&gt; new float[inputs.length]);</span>
<span class="source-line-no">092</span><span id="line-92">                    for (int i = 0; i &lt; inputs.length; i++) gradientSumPerWeight[i] += delta * inputs[i];</span>
<span class="source-line-no">093</span><span id="line-93">                    dBsum.merge(neuron, delta, Float::sum);</span>
<span class="source-line-no">094</span><span id="line-94">                }</span>
<span class="source-line-no">095</span><span id="line-95">            }</span>
<span class="source-line-no">096</span><span id="line-96">        }</span>
<span class="source-line-no">097</span><span id="line-97"></span>
<span class="source-line-no">098</span><span id="line-98">        // Apply averaged gradients: w := w - η * (1/N) * dWsum ; b := b - η * (1/N) * dBsum</span>
<span class="source-line-no">099</span><span id="line-99">        final float invN = 1f / sampleCount;</span>
<span class="source-line-no">100</span><span id="line-100">        for (int layer = 1; layer &lt;= depth; layer++) {</span>
<span class="source-line-no">101</span><span id="line-101">            int width = neuralNetwork.getWidth(layer);</span>
<span class="source-line-no">102</span><span id="line-102">            for (int j = 0; j &lt; width; j++) {</span>
<span class="source-line-no">103</span><span id="line-103">                ActivationsCachedNeuron neuron = neuralNetwork.getNeuron(layer, j, ActivationsCachedNeuron.class);</span>
<span class="source-line-no">104</span><span id="line-104"></span>
<span class="source-line-no">105</span><span id="line-105">                // We may have "dead" units in ReLU → if never used, skip</span>
<span class="source-line-no">106</span><span id="line-106">                float[] w = neuron.getWeights();</span>
<span class="source-line-no">107</span><span id="line-107">                float[] gw = dWsum.get(neuron);</span>
<span class="source-line-no">108</span><span id="line-108">                if (gw != null) {</span>
<span class="source-line-no">109</span><span id="line-109">                    for (int i = 0; i &lt; w.length; i++) {</span>
<span class="source-line-no">110</span><span id="line-110">                        w[i] -= learningRate * invN * gw[i];</span>
<span class="source-line-no">111</span><span id="line-111">                    }</span>
<span class="source-line-no">112</span><span id="line-112">                }</span>
<span class="source-line-no">113</span><span id="line-113">                Float gb = dBsum.get(neuron);</span>
<span class="source-line-no">114</span><span id="line-114">                if (gb != null) {</span>
<span class="source-line-no">115</span><span id="line-115">                    neuron.setBias(neuron.getBias() - learningRate * invN * gb);</span>
<span class="source-line-no">116</span><span id="line-116">                }</span>
<span class="source-line-no">117</span><span id="line-117"></span>
<span class="source-line-no">118</span><span id="line-118">                // clear per-batch caches</span>
<span class="source-line-no">119</span><span id="line-119">                neuron.clearCache();</span>
<span class="source-line-no">120</span><span id="line-120">            }</span>
<span class="source-line-no">121</span><span id="line-121">        }</span>
<span class="source-line-no">122</span><span id="line-122">    }</span>
<span class="source-line-no">123</span><span id="line-123">}</span>




























































</pre>
</div>
</main>
</body>
</html>
