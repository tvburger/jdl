<!DOCTYPE HTML>
<html lang>
<head>
<!-- Generated by javadoc (24) -->
<title>Source code</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="source: package: net.tvburger.jdl.model.training.loss, interface: BatchLossFunction">
<meta name="generator" content="javadoc/SourceToHTMLConverter">
<link rel="stylesheet" type="text/css" href="../../../../../../../resource-files/stylesheet.css" title="Style">
</head>
<body class="source-page">
<main role="main">
<div class="source-container">
<pre><span class="source-line-no">001</span><span id="line-1">package net.tvburger.jdl.model.training.loss;</span>
<span class="source-line-no">002</span><span id="line-2"></span>
<span class="source-line-no">003</span><span id="line-3">import net.tvburger.jdl.common.patterns.DomainObject;</span>
<span class="source-line-no">004</span><span id="line-4">import net.tvburger.jdl.common.patterns.Strategy;</span>
<span class="source-line-no">005</span><span id="line-5"></span>
<span class="source-line-no">006</span><span id="line-6">import java.util.List;</span>
<span class="source-line-no">007</span><span id="line-7"></span>
<span class="source-line-no">008</span><span id="line-8">/**</span>
<span class="source-line-no">009</span><span id="line-9"> * Represents a loss function that operates at the batch level,</span>
<span class="source-line-no">010</span><span id="line-10"> * i.e., across multiple samples. A batch is typically a collection</span>
<span class="source-line-no">011</span><span id="line-11"> * of samples used together in training for efficiency and stability</span>
<span class="source-line-no">012</span><span id="line-12"> * of gradient-based optimization.</span>
<span class="source-line-no">013</span><span id="line-13"> *</span>
<span class="source-line-no">014</span><span id="line-14"> * &lt;p&gt;Implementations of this interface compute both the total batch loss</span>
<span class="source-line-no">015</span><span id="line-15"> * and the gradient of the batch loss with respect to per-sample losses.&lt;/p&gt;</span>
<span class="source-line-no">016</span><span id="line-16"> *</span>
<span class="source-line-no">017</span><span id="line-17"> * &lt;p&gt;Definitions:&lt;/p&gt;</span>
<span class="source-line-no">018</span><span id="line-18"> * &lt;ul&gt;</span>
<span class="source-line-no">019</span><span id="line-19"> *   &lt;li&gt;&lt;b&gt;Batch loss:&lt;/b&gt; the aggregated loss over all samples in the batch.&lt;/li&gt;</span>
<span class="source-line-no">020</span><span id="line-20"> *   &lt;li&gt;&lt;b&gt;Gradient dJ/dL:&lt;/b&gt; the derivative of the batch loss with respect to</span>
<span class="source-line-no">021</span><span id="line-21"> *       the per-sample losses, used for backpropagation.&lt;/li&gt;</span>
<span class="source-line-no">022</span><span id="line-22"> * &lt;/ul&gt;</span>
<span class="source-line-no">023</span><span id="line-23"> */</span>
<span class="source-line-no">024</span><span id="line-24">@DomainObject</span>
<span class="source-line-no">025</span><span id="line-25">@Strategy(Strategy.Role.INTERFACE)</span>
<span class="source-line-no">026</span><span id="line-26">public interface BatchLossFunction extends LossFunction {</span>
<span class="source-line-no">027</span><span id="line-27"></span>
<span class="source-line-no">028</span><span id="line-28">    /**</span>
<span class="source-line-no">029</span><span id="line-29">     * Calculates the total loss for the batch by aggregating the per-sample losses.</span>
<span class="source-line-no">030</span><span id="line-30">     *</span>
<span class="source-line-no">031</span><span id="line-31">     * @param sampleLosses a list of individual sample losses</span>
<span class="source-line-no">032</span><span id="line-32">     * @return the total batch loss</span>
<span class="source-line-no">033</span><span id="line-33">     */</span>
<span class="source-line-no">034</span><span id="line-34">    float calculateBatchLoss(List&lt;Float&gt; sampleLosses);</span>
<span class="source-line-no">035</span><span id="line-35"></span>
<span class="source-line-no">036</span><span id="line-36">    /**</span>
<span class="source-line-no">037</span><span id="line-37">     * Computes the gradient of the batch loss with respect to the per-sample losses.</span>
<span class="source-line-no">038</span><span id="line-38">     * This is commonly denoted as dJ/dL in backpropagation equations.</span>
<span class="source-line-no">039</span><span id="line-39">     *</span>
<span class="source-line-no">040</span><span id="line-40">     * @param batchSize the number of samples in the batch</span>
<span class="source-line-no">041</span><span id="line-41">     * @return the gradient of the batch loss with respect to each sample loss</span>
<span class="source-line-no">042</span><span id="line-42">     */</span>
<span class="source-line-no">043</span><span id="line-43">    float calculateGradient_dJ_dL(int batchSize);</span>
<span class="source-line-no">044</span><span id="line-44"></span>
<span class="source-line-no">045</span><span id="line-45">}</span>




























































</pre>
</div>
</main>
</body>
</html>
