<!DOCTYPE HTML>
<html lang>
<head>
<!-- Generated by javadoc (24) -->
<title>Source code</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="source: package: net.tvburger.jdl.model.training.loss, class: MeanError">
<meta name="generator" content="javadoc/SourceToHTMLConverter">
<link rel="stylesheet" type="text/css" href="../../../../../../../resource-files/stylesheet.css" title="Style">
</head>
<body class="source-page">
<main role="main">
<div class="source-container">
<pre><span class="source-line-no">001</span><span id="line-1">package net.tvburger.jdl.model.training.loss;</span>
<span class="source-line-no">002</span><span id="line-2"></span>
<span class="source-line-no">003</span><span id="line-3">import net.tvburger.jdl.common.patterns.Strategy;</span>
<span class="source-line-no">004</span><span id="line-4"></span>
<span class="source-line-no">005</span><span id="line-5">import java.util.List;</span>
<span class="source-line-no">006</span><span id="line-6"></span>
<span class="source-line-no">007</span><span id="line-7">/**</span>
<span class="source-line-no">008</span><span id="line-8"> * A loss function implementation that computes the mean error</span>
<span class="source-line-no">009</span><span id="line-9"> * across samples or batches.</span>
<span class="source-line-no">010</span><span id="line-10"> *</span>
<span class="source-line-no">011</span><span id="line-11"> * &lt;p&gt;</span>
<span class="source-line-no">012</span><span id="line-12"> * {@code MeanError} provides a simple averaging strategy for loss</span>
<span class="source-line-no">013</span><span id="line-13"> * calculation. It can operate both at the sample level</span>
<span class="source-line-no">014</span><span id="line-14"> * ({@link SampleLossFunction}) and at the batch level</span>
<span class="source-line-no">015</span><span id="line-15"> * ({@link BatchLossFunction}), making it reusable in different</span>
<span class="source-line-no">016</span><span id="line-16"> * stages of the loss aggregation hierarchy.</span>
<span class="source-line-no">017</span><span id="line-17"> * &lt;/p&gt;</span>
<span class="source-line-no">018</span><span id="line-18"> *</span>
<span class="source-line-no">019</span><span id="line-19"> * &lt;p&gt;</span>
<span class="source-line-no">020</span><span id="line-20"> * At the sample level, this class typically averages the individual</span>
<span class="source-line-no">021</span><span id="line-21"> * dimension losses produced by {@link DimensionLossFunction}s.</span>
<span class="source-line-no">022</span><span id="line-22"> * At the batch level, it averages the losses of all samples within</span>
<span class="source-line-no">023</span><span id="line-23"> * the batch.</span>
<span class="source-line-no">024</span><span id="line-24"> * &lt;/p&gt;</span>
<span class="source-line-no">025</span><span id="line-25"> *</span>
<span class="source-line-no">026</span><span id="line-26"> * &lt;p&gt;</span>
<span class="source-line-no">027</span><span id="line-27"> * This implementation is often used as a building block in</span>
<span class="source-line-no">028</span><span id="line-28"> * higher-level objective functions such as Mean Squared Error (MSE)</span>
<span class="source-line-no">029</span><span id="line-29"> * or Binary Cross-Entropy (BCE), where mean aggregation is required</span>
<span class="source-line-no">030</span><span id="line-30"> * either per sample, per batch, or both.</span>
<span class="source-line-no">031</span><span id="line-31"> * &lt;/p&gt;</span>
<span class="source-line-no">032</span><span id="line-32"> *</span>
<span class="source-line-no">033</span><span id="line-33"> * @see SampleLossFunction</span>
<span class="source-line-no">034</span><span id="line-34"> * @see BatchLossFunction</span>
<span class="source-line-no">035</span><span id="line-35"> * @see DimensionLossFunction</span>
<span class="source-line-no">036</span><span id="line-36"> */</span>
<span class="source-line-no">037</span><span id="line-37">@Strategy(Strategy.Role.CONCRETE)</span>
<span class="source-line-no">038</span><span id="line-38">public class MeanError implements SampleLossFunction, BatchLossFunction {</span>
<span class="source-line-no">039</span><span id="line-39"></span>
<span class="source-line-no">040</span><span id="line-40">    /**</span>
<span class="source-line-no">041</span><span id="line-41">     * {@inheritDoc}</span>
<span class="source-line-no">042</span><span id="line-42">     */</span>
<span class="source-line-no">043</span><span id="line-43">    @Override</span>
<span class="source-line-no">044</span><span id="line-44">    public float calculateBatchLoss(List&lt;Float&gt; sampleLosses) {</span>
<span class="source-line-no">045</span><span id="line-45">        float loss = 0.0f;</span>
<span class="source-line-no">046</span><span id="line-46">        for (Float sampleLoss : sampleLosses) {</span>
<span class="source-line-no">047</span><span id="line-47">            loss += sampleLoss;</span>
<span class="source-line-no">048</span><span id="line-48">        }</span>
<span class="source-line-no">049</span><span id="line-49">        return loss / sampleLosses.size();</span>
<span class="source-line-no">050</span><span id="line-50">    }</span>
<span class="source-line-no">051</span><span id="line-51"></span>
<span class="source-line-no">052</span><span id="line-52">    /**</span>
<span class="source-line-no">053</span><span id="line-53">     * {@inheritDoc}</span>
<span class="source-line-no">054</span><span id="line-54">     */</span>
<span class="source-line-no">055</span><span id="line-55">    @Override</span>
<span class="source-line-no">056</span><span id="line-56">    public float calculateGradient_dJ_dL(int batchSize) {</span>
<span class="source-line-no">057</span><span id="line-57">        return 1.0f / batchSize;</span>
<span class="source-line-no">058</span><span id="line-58">    }</span>
<span class="source-line-no">059</span><span id="line-59"></span>
<span class="source-line-no">060</span><span id="line-60">    /**</span>
<span class="source-line-no">061</span><span id="line-61">     * {@inheritDoc}</span>
<span class="source-line-no">062</span><span id="line-62">     */</span>
<span class="source-line-no">063</span><span id="line-63">    @Override</span>
<span class="source-line-no">064</span><span id="line-64">    public float calculateSampleLoss(List&lt;Float&gt; dimensionLosses) {</span>
<span class="source-line-no">065</span><span id="line-65">        float loss = 0.0f;</span>
<span class="source-line-no">066</span><span id="line-66">        for (float dimensionLoss : dimensionLosses) {</span>
<span class="source-line-no">067</span><span id="line-67">            loss += dimensionLoss;</span>
<span class="source-line-no">068</span><span id="line-68">        }</span>
<span class="source-line-no">069</span><span id="line-69">        return loss / dimensionLosses.size();</span>
<span class="source-line-no">070</span><span id="line-70">    }</span>
<span class="source-line-no">071</span><span id="line-71"></span>
<span class="source-line-no">072</span><span id="line-72">    /**</span>
<span class="source-line-no">073</span><span id="line-73">     * {@inheritDoc}</span>
<span class="source-line-no">074</span><span id="line-74">     */</span>
<span class="source-line-no">075</span><span id="line-75">    @Override</span>
<span class="source-line-no">076</span><span id="line-76">    public float calculateGradient_dL_dl(int dimensions) {</span>
<span class="source-line-no">077</span><span id="line-77">        return 1.0f / dimensions;</span>
<span class="source-line-no">078</span><span id="line-78">    }</span>
<span class="source-line-no">079</span><span id="line-79"></span>
<span class="source-line-no">080</span><span id="line-80">}</span>




























































</pre>
</div>
</main>
</body>
</html>
